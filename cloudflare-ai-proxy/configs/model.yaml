# Default model (small, fast, cheap)
model_id: "@cf/meta/llama-3.2-1b-instruct"

# Inference parameters (optimized for speed and cost)
inference:
  max_tokens: 256
  temperature: 0.7
  top_p: 0.95
  
# Alternative models you can switch to:
# model_id: "@cf/mistral/mistral-7b-instruct-v0.1"
# model_id: "@cf/meta/llama-3.1-8b-instruct"
# model_id: "@cf/microsoft/phi-2"
